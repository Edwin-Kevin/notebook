# 将点云和视觉图像融合的难点：

- 视觉图像是在一个平面上的，而点云保留了三维信息；
- 点云是无规则、离散、无序的，图像则相反。

---

# 处理点云的方法

- 基于体积的表示法：将点云按照网格划分，并在每个网格中手工提取特征。但缺点是会丢失精度，并且导致性能下降。
- 基于索引/树的表示法：根据区域中点的密度动态调整分辨率，在提取点特征的同时预建立树形结构。
- 基于二维视图的表示法：将点云投影到多个二维视图平面。因此，可以通过投影得到的多个二维视图进行常规卷积学习，但此方法丢失了三维特征。
- 基于图：将点云图当作图，直接使用 MLP 对邻域中的点进行卷积，并在拉普拉斯域进行频谱滤波。
- **基于点**：使用 MLP 直接开发基于点的卷积运算。(e.g. **PointNet**)

    - `PointNet`: 采用独立的 `T-Net` 模块来对齐点云，并使用共享的 `MLP` 处理单个点来逐点提取特征。其计算复杂度随着输入点云的规模而线性增长，更具扩展性。  
    提取过程表达式：$ g({x_1, \cdots, x_n}) \approx f_{sym}(h(x_1), \cdots, h(x_n)) $ ，其中，$ x $ 代表输入点云，$ h $ 代表点云特征提取函数（例如 MLP），$ f_{sym} $ 代表对称函数（例如 最大池化），$ g $ 则为表示该运算的近似函数。
    - `PointNet++`: 将点分成多组并局部应用 PointNet 来提取不同层次的特征，来提取不同层次的特征。  
    - `RandLA-Net`:  堆叠随机点采样模块和基于注意力的局部特征聚合模块，在高效率的前提下增加感受野。
    
---

# 深度补全
**目的**：将稀疏的点上采样为密集的规则点，使得全局点云分布尽可能均匀。

## 将相机和点云融合的方法
此方法的思想是， RGB 图像也包含 3D 几何物体的一些形状信息，因此可以使用 RGB 颜色图像作为深度信息上采样补全的指导。

1. 信号级融合：  
[Ma 等人](https://ieeexplore.ieee.org/abstract/document/8793637) 提出一种基于模型的自监督框架，仅需要图像序列和稀疏深度图像来进行训练。但此方法假设物体是静止的，且产生的深度输出是模糊的。  
[CSPN](https://openaccess.thecvf.com/content_ECCV_2018/html/Xinjing_Cheng_Depth_Estimation_via_ECCV_2018_paper.html) 能够提取与图像相关的亲和力矩阵(Affinity Matrix)；  
[CSPN++](https://ojs.aaai.org/index.php/AAAI/article/view/6635) 则动态选择卷积核大小和迭代次数，以及加权组合来提高性能。
2. 特征级融合：
[Jaritz 等人](https://ieeexplore.ieee.org/abstract/document/8490955/)提出了一种自动编码器网络，可以对稀疏的深度图进行补全；
[Wang 等人](https://ieeexplore.ieee.org/abstract/document/8794404) 设计了一种可积分模块，利用稀疏深度图来提高现有基于图像的深度预测网络的性能。该 PnP 模块利用稀疏深度计算出的梯度来更新现有深度预测网络生成的中间特征图；
[Eldesokey 等人](https://ieeexplore.ieee.org/abstract/document/8765412) 提出了一个用于无引导深度完成的框架，该框架并行处理图像和非常稀疏的深度图，并在共享解码器中合并它们。此外，使用归一化卷积来处理高度稀疏的深度并传播置信度。
[Valada 等人](https://link.springer.com/article/10.1007/s11263-019-01188-y) 将 one-stage 特征级融合扩展到网络不同深度的多个阶段。
[GuideNet](https://ieeexplore.ieee.org/abstract/document/9286883) 在编码器的不同阶段将图像特征与稀疏深度特征融合，以引导稀疏深度的上采样。
3. 多层次融合：
[Van Gansbeke 等人](https://ieeexplore.ieee.org/abstract/document/8757939/) 在图像引导的深度完成网络中进一步结合了信号级融合和特征级融合。该网络由一个全局分支和一个局部分支组成，用于并行处理RGB-D数据和深度数据，在基于置信度图的情况下之前融合它们。

---

# 调研网络

![关于视觉和雷达数据融合的一些网络](overview.png)

## [LCNet](https://link.springer.com/article/10.1007/s10514-009-9113-3)

本文的模型可以用于检测地面上的多条车道，这是通过融合多个异步传感器流来实现的，并且对于障碍物等具有一定的健壮性。

LCNet 含有多个异构传感器，可以融合道路标点检测、障碍物检测、边缘检测等数据，实现中线追踪。

### 视觉线条匹配（使用滤波器滤出路面标点）

首先为图像的每一行配置一个一维滤波器，滤波器的宽度应与投影的画线标记的预期宽度一致。

首先使用水平和竖直算子提取边缘特征，标记离极值最近的点，并使用这些点拟合出线条。在拟合过程中，先选择离相机最近、最清晰的点，并在附近搜寻可能的匹配点，尝试沿着趋势**拟合曲线**，并计算其他可能点离曲线的距离，不断选择距离最近的点作为新的点，并重新绘制曲线，如此反复迭代，直到最后的点离趋势曲线较远，就结束寻找，记录曲线，并将涉及的特征点从待选点集合中删除，以免干扰下一条曲线的拟合。

对于较长的曲线，还会给予额外的得分奖励，鼓励向地平线延申的长曲线。

### 雷达探测

如果雷达回波高度有明显变化，则该位置存在垂直物体。

## [MSRF](https://www.sciencedirect.com/science/article/pii/S0020025517307119)

本文在条件混合场CRF(Conditional random field)的基础上，提出了一种新型混合CRF模型，用于融合摄像头和激光雷达信息。对齐激光雷达点和像素后，将像素和激光雷达点的标签（道路或背景）作为随机变量，并通过混合能量函数的最小化来推断标签。

大多数方法都是以摄像头或激光雷达为主导，未能充分发挥两种传感器的优势。例如，在 [43] 中，将激光雷达点云投影到图像上后，用于障碍物分类的特征主要是激光雷达点的高度信息，而忽略了像素信息。在 [21] 中，图像和激光雷达点云的信息是分阶段分别利用的。激光雷达点云仅用于地面种子提取，而接下来的道路检测和分割则以图像为主。在 [54] 中，融合是在特征和区域级别上进行的，结果是粗级别融合。所有这些方法都无法通过联合模型对图像和激光雷达进行细粒度融合。

### 图像和激光雷达点对齐

在激光雷达中的坐标点，需要将其转换为相机坐标。

$ p_c = \mathbf{R}_{rect} \mathbf{T}_{velo}^{cam} p $

其中，$ \mathbf{T}_{velo}^{cam} $ 是激光雷达坐标到相机坐标的变换矩阵， $ \mathbf{R}_{rect} $ 是旋转矫正矩阵。

在上一步变换后，z值为负的点被移除，剩下的点被投影矩阵 $ \mathbf{P}_{rect} $ 投射到图像平面上。

$ 
 \begin{bmatrix}
 u' & v' & w
 \end{bmatrix}^T = \mathbf{P}_{rect}
 \begin{bmatrix}
 x_c & y_c & z_c & 1
 \end{bmatrix}^T
$

在此例子中，投影出图像视场 FOV(field of view) 的点会被丢弃。

